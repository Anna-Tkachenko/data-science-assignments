{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DH1XQ07F5Bx8"
   },
   "source": [
    "# Introduction\n",
    "\n",
    "Welcome to our class on deep learning and neural networks.\n",
    "In this session, we will introduce basic concepts of neural networks, key parameters and concepts in deep learning, and how these models are used in financial tasks like creditworthiness prediction.\n",
    "\n",
    "## Agenda:\n",
    "1. Useful concepts\n",
    "2. Data preprocessing\n",
    "3. Building a neural network model\n",
    "4. Assignment overview\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h8AUO6PKRBCm"
   },
   "source": [
    "\n",
    "\n",
    "# 1. Useful concepts\n",
    "An **activation function** determines the output of a neuron given an input or set of inputs. They introduce non-linearity into the network, allowing it to learn from errors and improve.\n",
    "\n",
    "**Common activation functions:**\n",
    "| **Activation Function**    | **Description**                                                                                       |\n",
    "|----------------------------|-------------------------------------------------------------------------------------------------------|\n",
    "| **Linear**                 | Not so useful for deep learning; can't perform backpropagation, essential for training multi-layer networks. |\n",
    "| **Binary**                 | On/off activation; limited for complex classification; vertical slopes hinder calculus, ineffective for modern networks. |\n",
    "| **Non-linear** (key to DL) | Essential for complex mappings, backpropagation, and multiple layers. Examples below.  |\n",
    "\n",
    "\n",
    "**Common NON LINEAR activation functions:**\n",
    "| **Activation Function**    | **Description**                                                                                       |\n",
    "|----------------------------|-------------------------------------------------------------------------------------------------------|\n",
    "| **Sigmoid (Logistic)**      | Scales outputs between 0 and 1; common for binary classification but suffers from vanishing gradients. |\n",
    "| **Tanh (Hyperbolic)**       | Scales between -1 and +1; better for RNNs, but faces vanishing gradient issues and is computationally expensive. |\n",
    "| **ReLU**                    | Popular for simplicity and speed; becomes linear when <= 0, limiting learning.                        |\n",
    "| **Leaky ReLU**              | Introduces a negative slope for values below 0, solving the zeroing-out problem.                      |\n",
    "| **Parametric ReLU (PReLU)** | Similar to Leaky ReLU, but slope is learned via backpropagation; computationally intensive.            |\n",
    "| **Other ReLU Variants**     | Includes ELU (Exponential), Google’s Swish (deep networks), Maxout (powerful but impractical).         |\n",
    "| **Softmax**                 | Converts outputs to probabilities; used for single-label classification; Sigmoid handles multi-label tasks. |\n",
    "\n",
    "\n",
    "**Loss function:** Measures the difference between the predicted output and the actual target. The goal is to minimize this loss during training.\n",
    "  - Example: Mean Squared Error (MSE), Cross-Entropy Loss.\n",
    "  \n",
    "**Optimization algorithms:** Methods used to minimize the loss function by adjusting the network's weights.\n",
    "  - **Gradient Descent:** The most common optimization algorithm, which updates the model's parameters by computing the gradient of the loss function. \n",
    "  - **Adam** is a popular stochastic gradient descent method that is computationally efficient, has little memory requirement and is well suited for problems that are large in terms of data/parameters.\n",
    "  - **Learning Rate:** A hyperparameter that controls how much the model's parameters are adjusted with each step of the optimization.\n",
    "\n",
    "**Number of layers and neurons:** More layers and neurons can capture more complex patterns but may lead to overfitting if not managed properly.\n",
    "\n",
    "**Regularization:** Techniques like dropout or L2 regularization help prevent overfitting by adding constraints to the model.\n",
    "\n",
    "---\n",
    "\n",
    "## Popular Python Frameworks for Neural Networks\n",
    "\n",
    "### Overview of Popular Frameworks\n",
    "- **TensorFlow and Keras:** Developed by Google, Keras is a high-level API that runs on top of TensorFlow. It's user-friendly and great for beginners.\n",
    "- **PyTorch:** Developed by Facebook, PyTorch is popular in academia for its flexibility and ease of debugging.\n",
    "\n",
    "**For This Course:**  \n",
    "We’ll focus on TensorFlow/Keras due to its simplicity and wide adoption in the industry.\n",
    "\n",
    "---\n",
    "\n",
    "Demo: Implementation in Python\n",
    "------------------------------\n",
    "\n",
    "### LendingClub Use Case\n",
    "\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aeseLRulRBCn"
   },
   "source": [
    "### Set up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sVcRQbveRBCo"
   },
   "source": [
    "#### User-specified parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5TTTjZ1KRBCo"
   },
   "outputs": [],
   "source": [
    "python_material_folder_name = \"python-material\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o_8Z641LRBCp"
   },
   "source": [
    "#### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1624,
     "status": "ok",
     "timestamp": 1725040064057,
     "user": {
      "displayName": "Andrea Baroni",
      "userId": "13443912204230378793"
     },
     "user_tz": -60
    },
    "id": "_fnt6O1aRBCp",
    "outputId": "cae554fc-3efd-4fe3-d18c-fb5572b9f1e2"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "\n",
    "# Check if in Google Colab environment\n",
    "try:\n",
    "    from google.colab import drive\n",
    "    # Mount drive\n",
    "    drive.mount('/content/drive')\n",
    "    # Set up path to Python material parent folder\n",
    "    path_python_material = rf\"drive/MyDrive/{python_material_folder_name}\"\n",
    "        # If unsure, print current directory path by executing the following in a new cell:\n",
    "        # !pwd\n",
    "    IN_COLAB = True\n",
    "except:\n",
    "    IN_COLAB = False\n",
    "    # If working locally on Jupyter Notebook, parent folder is one folder up (assuming you are using the folder structure shared at the beginning of the course)\n",
    "    path_python_material = \"..\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aSq1v8uyRBCq"
   },
   "source": [
    "#### Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 226
    },
    "executionInfo": {
     "elapsed": 1314,
     "status": "ok",
     "timestamp": 1725039678627,
     "user": {
      "displayName": "Andrea Baroni",
      "userId": "13443912204230378793"
     },
     "user_tz": -60
    },
    "id": "xJviswQFRBCq",
    "outputId": "7ed26cfc-8d9a-4c84-c8fa-b683fbf50e90"
   },
   "outputs": [],
   "source": [
    "# Read data that was exported from previous session\n",
    "df = pd.read_csv(f\"{path_python_material}/data/2-intermediate/df_out_dsif6.csv\").sample(10000)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:RED\"> **>>> NOTE:**  </span>    \n",
    "> **Make sure to have plenty of data!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TGArDhlGRBCr"
   },
   "source": [
    "# 2. Data preprocessing\n",
    "\n",
    "Neural networks expect data in a specific format and often require transformations to perform effectively. Here's a brief overview.\n",
    "\n",
    "-   **Numerical Input Data:**\n",
    "    -   **Transformation:** Convert categorical data to numerical using techniques like one-hot encoding or label encoding. Convert categorical target labels to integers or one-hot encoded vectors.\n",
    "    -   **Why:** Neural networks can't process strings or categorical data directly.\n",
    "\n",
    "-   **Feature Scaling:**\n",
    "    -   **Transformation:** Apply normalization (e.g., min-max scaling) or standardization (e.g., z-score normalization).\n",
    "    -   **Why:** Neural networks converge faster and perform better when input features are on a similar scale, preventing dominance of features with larger magnitudes.\n",
    "    \n",
    "-   **Handling Missing Data:**\n",
    "    -   **Transformation:** Impute missing values (e.g., using mean/mode imputation) or remove rows/columns with missing data.\n",
    "    -   **Why:** Missing data can lead to inaccurate training or model errors if not addressed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gK5AkKObRBCr"
   },
   "outputs": [],
   "source": [
    "# Let's use same features used by model previously built, plus the categorical ones\n",
    "features = ['installment', 'revol_bal', 'recoveries', 'collection_recovery_fee',\n",
    "       'last_fico_range_high', 'last_fico_range_low', 'tot_cur_bal',\n",
    "       'open_acc_6m', 'open_il_24m', 'total_bal_il', 'inq_fi',\n",
    "       'acc_open_past_24mths', 'bc_util', 'mo_sin_old_il_acct',\n",
    "       'percent_bc_gt_75', 'total_il_high_credit_limit', 'last_pymnt_amnt_log',\n",
    "       'last_pymnt_amnt_capped', 'grade_encoded', 'annual_inc_std']\n",
    "\n",
    "X = df[features]\n",
    "y = df['loan_default']\n",
    "\n",
    "print(f\"Number of features: {len(features)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Apply Min-Max Scaling (alternative: StandardScaler for z-score normalization)\n",
    "scaler = MinMaxScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Convert back to DataFrame with same column names\n",
    "X_scaled_df = pd.DataFrame(X_scaled, columns=features)\n",
    "X_scaled_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What can you observe now?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pMUETYxNRBCr"
   },
   "source": [
    "# 3. Building a neural network model\n",
    "\n",
    "We'll use TensorFlow/Keras, and refer to a simple parallel with the brain in the commentary to explain what is going on.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Fcke8q1eRBCs"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the data into training and testing sets (returns pandas dfs)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled_df, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Starting with a simple architecture\n",
    "\n",
    "### Dense relu -> Dense relu -> Dense sigmoid\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5838,
     "status": "ok",
     "timestamp": 1725039685917,
     "user": {
      "displayName": "Andrea Baroni",
      "userId": "13443912204230378793"
     },
     "user_tz": -60
    },
    "id": "LcjWbxKoRBCt",
    "outputId": "bc1b17dd-c852-41e1-ea4c-96bc424450f1"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "# Initialize the model (empty intiially)\n",
    "model = Sequential()\n",
    "\n",
    "# Adding layers to our model, which are like different parts of the brain.\n",
    "# Each layer has \"neurons\" (like tiny decision-makers):\n",
    "\n",
    "# Add the first part of our brain that looks at the data\n",
    "model.add(Dense(16, input_dim=X_scaled_df.shape[1], activation='relu'))\n",
    "\n",
    "# smaller brain part that processes what the first layer has figured out.\n",
    "model.add(Dense(8, activation='relu'))\n",
    "\n",
    "#This is like the brain's decision-making part, where it makes a yes/no decision (like \"Will this person pay back their loan?\").\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(\n",
    "    optimizer='adam', # like the brain’s coach, guiding it to get better and better at its task\n",
    "    loss='binary_crossentropy', # how the brain measures its mistakes,  for optimisation\n",
    "    metrics=['accuracy', 'Precision', 'Recall', 'AUC']\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5226,
     "status": "ok",
     "timestamp": 1725039691125,
     "user": {
      "displayName": "Andrea Baroni",
      "userId": "13443912204230378793"
     },
     "user_tz": -60
    },
    "id": "lsrq8x40RBCv",
    "outputId": "d6ae733b-7c38-46f8-d09e-13d9c88fa5fb"
   },
   "outputs": [],
   "source": [
    "# Train the model\n",
    "history = model.fit(X_train, y_train, epochs=10, batch_size=32, validation_split=0.2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mZ306jNCRBCw"
   },
   "source": [
    "#### What is going on here?\n",
    "-   **Epochs (10):** The brain goes through all the data 10 times, learning a bit more with each pass.  \n",
    "-   **Batch Size (32):** The brain practices on 32 examples at a time.\n",
    "-   **Validation Split (0.2):** We set aside 20% of the data to test the brain and see how well it's learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 30,
     "status": "ok",
     "timestamp": 1725039691126,
     "user": {
      "displayName": "Andrea Baroni",
      "userId": "13443912204230378793"
     },
     "user_tz": -60
    },
    "id": "uWjAyyThRBCx",
    "outputId": "3302da3c-9cb7-45b1-c146-b8954c742586"
   },
   "outputs": [],
   "source": [
    "# How does this compare to model built in previous sessions?\n",
    "\n",
    "# Make predictions\n",
    "y_prob = model.predict(X_test)\n",
    "y_pred = (y_prob > 0.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 495,
     "status": "ok",
     "timestamp": 1725040069622,
     "user": {
      "displayName": "Andrea Baroni",
      "userId": "13443912204230378793"
     },
     "user_tz": -60
    },
    "id": "g1oZYaQdRBCx",
    "outputId": "ba82433b-fa86-4097-f1f0-acab134210f0"
   },
   "outputs": [],
   "source": [
    "from dsif6utility import model_evaluation_report\n",
    "model_evaluation_report(X_test, y_test, y_pred, y_prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpreting outputs produced\n",
    "Let's look at the probability scores produced:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_prob = pd.DataFrame(y_prob, columns=[\"prob\"])\n",
    "\n",
    "y_prob.prob.describe(percentiles = [i / 100 for i in [0, 1, 10, 25, 50, 75, 90, 95, 99, 100]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:BLUE\"> **>>> DISCUSSION:**  </span>    \n",
    "> What do you notice?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To achieve a wider range of probabilities in your neural network output, you can consider the following modifications:\n",
    "\n",
    "**Change the output layer activation function**\n",
    "If your task requires predicting probabilities for multiple classes, use the `softmax` activation function instead of `sigmoid`. For binary classification, `sigmoid` is appropriate, but if you're observing outputs mostly at the extremes (0 or 1), consider adjusting the model complexity.\n",
    "\n",
    "**Increase model complexity**\n",
    "You can add more layers or increase the number of neurons in the existing layers to allow the model to learn more complex patterns.\n",
    "\n",
    "**Regularization techniques**\n",
    "Consider adding dropout layers to prevent overfitting, which can sometimes lead to extreme outputs.\n",
    "\n",
    "**Adjust the loss function**\n",
    "If your output is indeed binary but you want probabilities that are not strictly 0 or 1, ensure that you balance your dataset or adjust class weights during training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Some pointers/examples for NNs architectures\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following table includes a brief description of each architecture's structure and its rationale, providing a clearer understanding of why each architecture is suited for its respective use case.\n",
    "\n",
    "| Neural Network Type | Structure | Use Case | Links | Description |\n",
    "| --- | --- | --- | --- | --- |\n",
    "| **Basic Feedforward Neural Network** | Dense (ReLU) → Dense (ReLU) → Dense (Sigmoid) | Binary Classification | [Sonar, Mines vs. Rocks example](https://machinelearningmastery.com/binary-classification-tutorial-with-the-keras-deep-learning-library/) | Simple architecture for binary classification; fully connected layers enable learning complex relationships. |\n",
    "| **Simple Neural Network for Classification** | Dense (ReLU) → Dense (Softmax) | Multi-Class Classification | [Iris Dataset example](https://lnwatson.co.uk/posts/intro_to_nn/) | Designed for multi-class classification; softmax activation allows for probability distribution across multiple classes. |\n",
    "| **Convolutional Neural Network (CNN)** | Conv2D (ReLU) → flatten → Dense (Softmax) | Image Classification | [mnist image recognition example](https://towardsdatascience.com/building-a-convolutional-neural-network-cnn-in-keras-329fbbadc5f5) | Effective for image data; convolutional layers capture spatial hierarchies, while pooling reduces dimensionality. |\n",
    "| **Recurrent Neural Network (RNN)** | LSTM → Dense (Sigmoid) | Time Series Prediction | [lstm for time series example](https://machinelearningmastery.com/time-series-prediction-lstm-recurrent-neural-networks-python-keras/) | Suitable for sequential data; LSTMs manage long-term dependencies in time series or text inputs. |\n",
    "| **Autoencoder** | Dense (ReLU) → Dense (ReLU) → Dense (Bottleneck) → Dense (ReLU) → Dense (Sigmoid) | Unsupervised Learning, Feature Extraction, Fraud detection | [Autoencoders example](https://blog.keras.io/building-autoencoders-in-keras.html) | Unsupervised learning for feature extraction; compresses input data into a lower-dimensional representation. |\n",
    "| **Generative Adversarial Network (GAN)** | Dense (ReLU) → Dense (Output) (Generator) and Dense (ReLU) → Dense (Output) (Discriminator) | Synthetic data generation, Fraud detection | [GANs example](https://medium.com/@marcodelpra/generative-adversarial-networks-dba10e1b4424) | Two competing networks create new data; the generator learns to produce realistic samples while the discriminator assesses them. |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Calculating sample size required - Rule of thumb\n",
    "\n",
    "**Rule of 10**, namely the amount of training data you need for a well performing model is 10x the number of parameters in the model. ([source](https://malay-haldar.medium.com/how-much-training-data-do-you-need-da8ec091e956#:~:text=This%20leads%20us%20to%20the,of%20parameters%20in%20the%20model.))\n",
    "\n",
    "To determine the appropriate sample size based on the rule of 10, we first need to calculate the total number of parameters in your model. Here's how to do that step-by-step:\n",
    "\n",
    "### i \\. **Calculate Parameters for Each Layer**\n",
    "\n",
    "For a `Dense` layer, the number of parameters can be calculated as:\n",
    "\n",
    "Parameters=(number of inputs+1)×(number of neurons)\n",
    "\n",
    "-   **Input Layer:** The first layer has `input_dim` neurons, which is the number of features in your dataset.\n",
    "-   **Hidden Layer 1:** Has 16 neurons.\n",
    "-   **Hidden Layer 2:** Has 8 neurons.\n",
    "-   **Output Layer:** Has 1 neuron.\n",
    "\n",
    "### ii \\. **Calculate the Total Parameters**\n",
    "\n",
    "Assuming your input has n features:\n",
    "\n",
    "-   **First Layer:** (n+1)×16  \n",
    "-   **Second Layer:** (16+1)×8= 136  \n",
    "-   **Output Layer:** (8+1)×1= 9  \n",
    "\n",
    "### iii \\. **Putting It Together**\n",
    "\n",
    "#### Total Parameters:  \n",
    "To ensure a well-performing model, your sample size should be at least:  \n",
    "\n",
    "Sample Size = 10 * [(n+1)×16 + 136 + 9]\n",
    "\n",
    "\n",
    "\n",
    "### Example Calculation\n",
    "\n",
    "If you have, for example, 20 features:\n",
    "\n",
    "Sample Size = 10 * [(20+1)×16 + 136 + 9] = 4810\n",
    "\n",
    "So, if your input has 20 features, you should aim for at least **4810 samples** for training your model. Adjust the calculation based on the actual number of features in your dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OBRyKZrpRBCy"
   },
   "source": [
    "## 3.4 Detecting and dealing with overfitting in NNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training_vs_overfitting(history):\n",
    "    \"\"\"Plot training and validation accuracy to detect overfitting (when gap between 2 is detected)\"\"\"\n",
    "    import matplotlib.pyplot as plt\n",
    "    \n",
    "    # Plot accuracy\n",
    "    plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
    "    plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    # Plot loss\n",
    "    plt.plot(history.history['loss'], label='Train Loss')\n",
    "    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "plot_training_vs_overfitting(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sB5O49e8RBCy"
   },
   "source": [
    "### <span style=\"color:BLUE\"> **>>> DISCUSSION:**  </span>    \n",
    "> How can we see if there is any suspected overfitting? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Common techniques to deal with overfitting:\n",
    "\n",
    "#### i\\. **Add a Dropout Layer**\n",
    "\n",
    "Dropout is a regularization technique that randomly \"drops out\" a fraction of the neurons during training, forcing the model to learn more robust features.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dropout\n",
    "\n",
    "# Initialize the model\n",
    "model2 = Sequential()\n",
    "\n",
    "# First hidden layer with Dropout\n",
    "model2.add(Dense(16, input_dim=X_scaled_df.shape[1], activation='relu'))\n",
    "model2.add(Dropout(0.5))  # Drop 50% of neurons\n",
    "\n",
    "# Second hidden layer with Dropout\n",
    "model2.add(Dense(8, activation='relu'))\n",
    "model2.add(Dropout(0.5))  # Drop 50% of neurons\n",
    "\n",
    "# Output layer\n",
    "model2.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compile the model\n",
    "model2.compile(\n",
    "    optimizer='adam',\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy', 'Precision', 'Recall', 'AUC']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "history2 = model2.fit(X_train, y_train, epochs=10, batch_size=32, validation_split=0.2)\n",
    "plot_training_vs_overfitting(history2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2\\. **Early Stopping**\n",
    "\n",
    "Early stopping halts training once the performance on the validation data stops improving. This prevents the model from overfitting after a certain point.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# Early stopping to prevent overfitting\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_precision',  # Watch the validation loss\n",
    "    patience=5,  # Stop if no improvement after 5 epochs\n",
    "    restore_best_weights=True  # Restore the model weights at the best epoch\n",
    ")\n",
    "\n",
    "# Train with early stopping\n",
    "history3 = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_test, y_test),\n",
    "    epochs=50,\n",
    "    batch_size=32,\n",
    "    callbacks=[early_stopping]\n",
    ")\n",
    "plot_training_vs_overfitting(history3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3\\. **Reduce Model Complexity**\n",
    "Sometimes, your model may be too complex (too many layers or neurons). You can simplify it by reducing the number of hidden layers or neurons per layer.\n",
    "\n",
    "#### 4\\. **Data Augmentation**\n",
    "If possible, increase the amount of training data by augmenting it. More data can help the model generalize better, especially in tasks like image recognition (though it's less applicable for tabular data).\n",
    "\n",
    "#### 5\\. **Regularization (L2)**\n",
    "L2 regularization (also called weight decay) adds a penalty for large weights, which helps prevent overfitting.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.regularizers import l2\n",
    "\n",
    "# Add L2 regularization to the model\n",
    "model4 = Sequential()\n",
    "model4.add(Dense(16, input_dim=X_scaled_df.shape[1], activation='relu', kernel_regularizer=l2(0.01)))\n",
    "model4.add(Dense(8, activation='relu', kernel_regularizer=l2(0.01)))\n",
    "model4.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model4.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy', 'Precision', 'Recall', 'AUC'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "history4 = model4.fit(X_train, y_train, epochs=10, batch_size=32, validation_split=0.2)\n",
    "plot_training_vs_overfitting(history4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-jJGaVL_RBCz"
   },
   "source": [
    "# 4. Assignment overview\n",
    "\n",
    "### Part 1 - Mandatory\n",
    "**Develop a challenger model:** \\\n",
    "Using the concepts from **Session 7** and **Session 8**, create a challenger model to compare against the `model` and `model_2` built in **Session 5**. Name your new model `model_3`. Explore various architectures, models, activation functions, and other hyperparameters to improve performance.\n",
    "\n",
    "> Experiment with different combinations of optimizers, loss functions, and metrics to evaluate how they affect training speed, overall performance, and the final model's accuracy. This will help you fine-tune your model for the specific problem you're addressing.\n",
    "\n",
    "> Add at least one additional hidden layer. Observe how this affects the model's performance and overfitting and make note of the differences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "POC6twlhRBC0"
   },
   "source": [
    "# End of session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 350
    },
    "executionInfo": {
     "elapsed": 591,
     "status": "ok",
     "timestamp": 1725040081880,
     "user": {
      "displayName": "Andrea Baroni",
      "userId": "13443912204230378793"
     },
     "user_tz": -60
    },
    "id": "KuGRHlSsRBC0",
    "outputId": "b341871c-340e-4a78-dc53-205d28ee1b2a"
   },
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "Image(filename=f\"{path_python_material}/images/the-end.jpg\", width=500,)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5xciZDwtRBC0"
   },
   "source": [
    "# Appendix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8mgAjRWMRBCu"
   },
   "source": [
    "## Best practices for choosing activation functions:\n",
    "\n",
    "-   **ReLU (Rectified Linear Unit)**:\n",
    "\n",
    "    -   **Default choice for hidden layers** in most neural networks.\n",
    "    -   **Fast and simple**, helps avoid the vanishing gradient problem.\n",
    "    -   Use with caution if inputs can be negative or if your model isn't learning well (may cause \"dying ReLUs\").\n",
    "-   **Leaky ReLU**:\n",
    "\n",
    "    -   **Use when ReLU is causing dead neurons** (outputs stuck at 0).\n",
    "    -   Allows a small, non-zero gradient when the input is negative.\n",
    "-   **Sigmoid**:\n",
    "\n",
    "    -   Good for **binary classification output layers** (produces a probability between 0 and 1).\n",
    "    -   Avoid in hidden layers due to the **vanishing gradient problem**.\n",
    "-   **Tanh (Hyperbolic Tangent)**:\n",
    "\n",
    "    -   Use when outputs need to range between **-1 and 1**.\n",
    "    -   **Better than Sigmoid** for hidden layers but still prone to vanishing gradients.\n",
    "-   **Softmax**:\n",
    "\n",
    "    -   Ideal for **multi-class classification output layers**.\n",
    "    -   Outputs a probability distribution over multiple classes (sums to 1).\n",
    "\n",
    "-   **Linear**:\n",
    "\n",
    "    -   Use in the **output layer for regression tasks** (predicting continuous values).\n",
    "    -   No non-linearity is applied, so the output is a linear combination of inputs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Changing Optimizer, Loss, and Metrics: What to Expect\n",
    "\n",
    "Changing the **optimizer**, **loss**, and **metrics** in a neural network can have a significant impact on model performance, convergence speed, and the accuracy of predictions.\n",
    "\n",
    "More info here: https://www.tensorflow.org/api_docs/python/tf/keras\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
